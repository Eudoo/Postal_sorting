Étape 1 : Classification des chiffres

1.1 Apprentissage hors-ligne
✅ Question 1 : Espace de décision (10 features : surfaces, connectivité, barycentres)
✅ Question 2 : Apprentissage complet (X: 50×10, y: 50, centroïdes: 10×10)

1.2 Classification en-ligne
✅ Question 3 : k-NN (k=3) — distance euclidienne + vote majoritaire
✅ Question 4 : Validation → 94% digits, 70% codes postaux

Étape 2 : Améliorations

✅ Question 5 : Classification par plus proche moyenne (centroïdes)
  - predict_centroid_knn implémenté dans knn_utils.py
  - Calcul des centroïdes à chaque labeling

✅ Question 6 : Comparer k-NN vs centroïdes
  - Résultats identiques : 94% digits, 70% codes
  - Mode 'both' pour comparaison côte à côte
  - python inference.py --mode both

✅ Question 7 : Ajouter descripteurs invariants échelle via regionprops
  - Solidité (solidity) = aire_contour / aire_convexhull
  - Feature vector : 11 features (10 cavités + solidity)
  - Résultats :
    • k-NN (k=3) : 94% digits, 70% codes (inchangé)
    • Centroïde : 96% digits, 80% codes (+2% / +10%)
    • Correction 5→6 grâce à la solidité
  - python labeling.py && python inference.py --mode both

❌ Question 8 : Recalage en rotation
  - Mesurer l'angle de l'axe principal
  - Redresser l'image avant traitement

❌ Question 9 : Séparation de chiffres qui se touchent
  - Watershed ou morphologie (erosion→dilatation)




modifie le readme pour indexe quelle fonctionnalité correspond au resultat de la question donné hisotire de savoir que telle chose est la reponse a la question Q... 

